{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## To make the libraries you have uploaded in the Library Manager available in this Notebook, run the command below to get started\n",
    "\n",
    "```run -i platform-libs/initialize.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are all set to explore!!!\n"
     ]
    }
   ],
   "source": [
    "run -i platform-libs/initialize.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG]:  Adding config - Reader Host = 172.16.109.117:9998\n",
      "[DEBUG]:  Adding config - Recipe Builder Host = 172.16.109.117:8052\n",
      "[DEBUG]:  Config file /home/jovyan/work/data/config_notebook.json found! SDK configured.\n",
      "[DEBUG]:  Using configuration: \n",
      "{\n",
      "    \"ADMIN_MODULE\": {\n",
      "        \"IP\": \"172.16.109.117\",\n",
      "        \"PORT\": \"9101\"\n",
      "    },\n",
      "    \"AUTH_TOKEN_ENDPOINTS\": {\n",
      "        \"ADMIN_APP_URL\": \"http://172.16.109.117:9101/\"\n",
      "    },\n",
      "    \"CONSOLE_LOGGING_ENABLED\": true,\n",
      "    \"CONTROLLER_LOG_PATH\": \"/bigbrain/data/logs/\",\n",
      "    \"CONTROLLER_PORT\": 9094,\n",
      "    \"DEV_MODE\": true,\n",
      "    \"GRAPH_APP_ID\": \"engine\",\n",
      "    \"GRAPH_SCHEMA\": {\n",
      "        \"LINE_GRAPH\": \"LINE_GRAPH_v1.1\"\n",
      "    },\n",
      "    \"GRAYLOG_ENABLED\": false,\n",
      "    \"JAVA_CONTROLLER_PATH\": \"/bigbrain/data/java-controller/java-controller-3.0-SNAPSHOT-jar-with-dependencies.jar\",\n",
      "    \"KAFKA_BROKER\": {\n",
      "        \"HOSTS\": [\n",
      "            \"172.16.109.117:9092\"\n",
      "        ]\n",
      "    },\n",
      "    \"KAFKA_MANAGER_HOST\": \"172.16.109.117\",\n",
      "    \"KAFKA_MANAGER_PORT\": 8088,\n",
      "    \"LOGGING_SERVER_GELF_PORT\": 12201,\n",
      "    \"LOGGING_SERVER_HOST\": \"10.9.112.239\",\n",
      "    \"LOGGING_SERVER_PORT\": 12201,\n",
      "    \"LOG_LEVEL\": \"INFO\",\n",
      "    \"METRICS_SERVER_HOST\": \"10.9.32.43\",\n",
      "    \"METRICS_SERVER_PORT\": 8055,\n",
      "    \"METRIC_ID\": \"CPU-MEM vs TIME\",\n",
      "    \"MODEL_LIFE_CYCLE_MANAGEMENT_HOST\": \"10.9.32.43\",\n",
      "    \"MODEL_LIFE_CYCLE_MANAGEMENT_PORT\": \"8050\",\n",
      "    \"READER\": {\n",
      "        \"IP\": \"172.16.109.117\",\n",
      "        \"PORT\": \"9998\"\n",
      "    },\n",
      "    \"RECIPE_BUILDER\": {\n",
      "        \"IP\": \"172.16.109.117\",\n",
      "        \"PORT\": \"8052\"\n",
      "    },\n",
      "    \"SPARK\": {\n",
      "        \"EXECUTOR-CORE\": 2,\n",
      "        \"EXECUTOR-MEMORY\": \"2G\",\n",
      "        \"MASTER_HOST\": \"local[*]\"\n",
      "    }\n",
      "}\n",
      "[DEBUG]:  Using reader: 172.16.109.117:9998\n",
      "{\n",
      "    \"class_name\": null,\n",
      "    \"is_valid\": true,\n",
      "    \"error_list\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pipelineblocksdk.construct.base.StreamBlock import StreamBlock\n",
    "from pipelineblocksdk.api.Singleton import Singleton\n",
    "from pipelineblocksdk.data.spark.SparkConfCustom import SparkConfCustom\n",
    "from pipelineblocksdk.util.async_util import async\n",
    "from pipelineblocksdk.util.kerbUtil import generate_ticket_granting_ticket\n",
    "from pipelineblocksdk.util.ThirdPartyIntegration import get_oracle_creds\n",
    "\n",
    "from hdfs import InsecureClient\n",
    "from hdfs.ext.kerberos import KerberosClient\n",
    "\n",
    "from shutil import rmtree\n",
    "from ast import literal_eval\n",
    "\n",
    "import time\n",
    "import uuid\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pathlib\n",
    "import traceback\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "# Following code is the definition for a batch block\n",
    "\n",
    "class MyBlock(StreamBlock, Singleton):\n",
    "    base_temp_path = '/tmp/'\n",
    "    temp_file_paths = []\n",
    "    hdfs_client = None\n",
    "    spark = None\n",
    "    kafka_api_instance = None\n",
    "    left_df = None\n",
    "    right_df = None\n",
    "    join_df = None\n",
    "    output_topic = None\n",
    "    part_files = None\n",
    "\n",
    "    # This is the entry point for the block. This is a mandatory function.\n",
    "    def run(self):\n",
    "        t1 = time.time()\n",
    "        output_dict = dict()\n",
    "        self.logger.info('Run function says: SparkJoin')\n",
    "        self.logger.info(f\"Inputs available: {self.input_dict}\")\n",
    "        os.makedirs(self.base_temp_path, exist_ok=True)\n",
    "        self.kafka_api_instance = self.data_handler.api_instance\n",
    "\n",
    "        self.spark = SparkConfCustom(self.input_dict[\"SparkConf\"]).get_spark_session()\n",
    "        self.spark.sparkContext.setLogLevel('INFO')\n",
    "        try:\n",
    "            left_path, left_thread, right_path, right_thread = None, None, None, None\n",
    "            if self.input_dict['LeftDataSource']['queueTopicName'] \\\n",
    "                    and len(self.input_dict['LeftDataSource']['queueTopicName']) > 3:\n",
    "                left_path, left_thread = self.get_streaming_path(self.input_dict['LeftDataSource'])\n",
    "\n",
    "            if self.input_dict['RightDataSource']['queueTopicName'] \\\n",
    "                    and len(self.input_dict['RightDataSource']['queueTopicName']) > 3:\n",
    "                right_path, right_thread = self.get_streaming_path(self.input_dict['RightDataSource'])\n",
    "\n",
    "            # Waiting for streams to download the data to local disk\n",
    "            if left_thread:\n",
    "                left_thread.join()\n",
    "\n",
    "            if right_thread:\n",
    "                right_thread.join()\n",
    "\n",
    "            self.logger.info(self.input_dict['DataTarget']['stream'])\n",
    "\n",
    "            if ('type' in self.input_dict['LeftDataSource'] and self.input_dict['LeftDataSource']['type'] == 'hdfs') \\\n",
    "                    or ('type' in self.input_dict['RightDataSource'] and self.input_dict['RightDataSource'][\n",
    "                'type'] == 'hdfs'):\n",
    "                self.hdfs_client = self.get_client(self.block_params, self.input_dict['ConnectionParams'])\n",
    "\n",
    "            if (self.input_dict['DataTarget']['stream'] is not True) :\n",
    "                self.hdfs_client = self.get_client(self.block_params, self.input_dict['ConnectionParams'])\n",
    "\n",
    "            self.left_df = self.get_df(self.spark, self.input_dict['LeftDataSource'], self.block_params, left_path)\n",
    "            self.logger.info(str(self.left_df.schema.json()))\n",
    "            self.right_df = self.get_df(self.spark, self.input_dict['RightDataSource'], self.block_params, right_path)\n",
    "            self.logger.info(str(self.right_df.schema.json()))\n",
    "            self.spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "            self.right_df.show()\n",
    "\n",
    "            self.left_df.createOrReplaceTempView('left')\n",
    "            self.right_df.createOrReplaceTempView('right')\n",
    "\n",
    "            sql_query = self.input_dict['DataTarget']['join_query']\n",
    "            self.logger.info(f\"Query is {sql_query}\")\n",
    "            self.join_df = self.spark.sql(sql_query)\n",
    "\n",
    "            # Rename duplicate columns\n",
    "            col_list = self.join_df.columns\n",
    "            for col in col_list:\n",
    "                count = col_list.count(col)\n",
    "                if count > 1:\n",
    "                    idx = col_list.index(col)\n",
    "                    col_list[idx] = col + '_1'\n",
    "            self.logger.info(self.join_df.columns)\n",
    "            self.join_df = self.join_df.toDF(*col_list)\n",
    "            self.logger.info(self.join_df.columns)\n",
    "\n",
    "\n",
    "            # Write df to HDFS or write to file\n",
    "            join_time_st = time.time()\n",
    "            if self.input_dict['ConnectionParams']['kerberos'] == 'false' \\\n",
    "                    and str(self.input_dict['DataTarget']['filePath']).endswith('.parquet'):\n",
    "                # Write to HDFS\n",
    "                exists = self.file_exits(self.hdfs_client, self.input_dict['DataTarget']['filePath'])\n",
    "                if exists:\n",
    "                    if self.input_dict['DataTarget']['overwrite'] is True:\n",
    "                        # remove file\n",
    "                        self.delete_file(self.hdfs_client, self.input_dict['DataTarget']['filePath'])\n",
    "                    else:\n",
    "                        raise FileExistsError(\"File Already Exists: \" + str(self.input_dict['DataTarget']['filePath']))\n",
    "                host = self.input_dict['ConnectionParams']['hostName']\n",
    "                port = self.input_dict['ConnectionParams']['port']\n",
    "                target_path = self.input_dict['DataTarget']['filePath']\n",
    "                protocol = 'https://' if self.input_dict['ConnectionParams']['https'] == 'true' else 'http://'\n",
    "                hdfs_path = protocol + host + port + '/' + target_path\n",
    "                self.join_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(hdfs_path)\n",
    "                join_time_end = time.time()\n",
    "            else:\n",
    "                # Write to file and upload to HDFS\n",
    "                temp_fp = self.base_temp_path + str(t1) + '.csv'\n",
    "                self.join_df.write.mode(\"overwrite\").option(\"header\", \"false\").csv(temp_fp)\n",
    "                join_time_end = time.time()\n",
    "                # combine to single file\n",
    "                combine_start_time = time.time()\n",
    "                files = list(pathlib.Path(temp_fp).glob('*.csv'))\n",
    "                files.sort()\n",
    "                if self.input_dict['DataTarget']['stream'] == 'true' or self.input_dict['DataTarget']['stream'] is True:\n",
    "                    self.output_topic, self.producer = self.data_handler.create_producer(str(uuid.uuid4()))\n",
    "                    self.part_files = files\n",
    "                    self.stream()\n",
    "                    output_dict[\"queueTopicName\"] = self.output_topic\n",
    "                    return output_dict\n",
    "\n",
    "                exists = self.file_exits(self.hdfs_client, self.input_dict['DataTarget']['filePath'])\n",
    "                if exists:\n",
    "                    if self.input_dict['DataTarget']['overwrite'] is True:\n",
    "                        # remove file\n",
    "                        self.delete_file(self.hdfs_client, self.input_dict['DataTarget']['filePath'])\n",
    "                    else:\n",
    "                        raise FileExistsError(\"File Already Exists: \" + str(self.input_dict['DataTarget']['filePath']))\n",
    "                single_out_file = self.base_temp_path + str(combine_start_time) + '.csv'\n",
    "                self.temp_file_paths.append(single_out_file)\n",
    "                with open(single_out_file, 'a') as out_file:\n",
    "                    out_file.write(\",\".join(self.join_df.columns))\n",
    "                    for f in files:\n",
    "                        self.logger.info(f'{f} appended to single csv')\n",
    "                        with open(str(f), 'r') as in_file:\n",
    "                            for line in in_file:\n",
    "                                if len(line)>1:\n",
    "                                    out_file.write('\\n' + line)\n",
    "                            self.logger.info(f'Last line is: {line}')\n",
    "\n",
    "                print(\n",
    "                    f'Done with combining part files to a single file.\\n Time taken: {time.time() - combine_start_time}')\n",
    "\n",
    "                write_start_time = time.time()\n",
    "                self.logger.info(\"Writing to HDFS:\")\n",
    "                self.block_folder_write(self.hdfs_client, single_out_file, self.input_dict['DataTarget']['filePath']\n",
    "                                        , self.input_dict['DataTarget']['overwrite'])\n",
    "                self.logger.info(\"Time taken to write to HDFS: \" + str(time.time() - write_start_time))\n",
    "                self.logger.info('Time for Join: ' + str(join_time_end - join_time_st))\n",
    "        except Exception as e:\n",
    "            some_track = traceback.format_exc()\n",
    "            self.logger.error(f\"Error: {str(e)} \\n trace: {some_track}\")\n",
    "            self.logger.error(e)\n",
    "            raise e\n",
    "\n",
    "        self.clean_temp_files()\n",
    "        # Set the output parameter\n",
    "        output_dict['file_path'] = self.input_dict['DataTarget']['filePath']\n",
    "        # Set the status of the block as completed\n",
    "        self.block_status = \"COMPLETED\"\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    def clean_temp_files(self):\n",
    "        self.logger.info(\"Cleaning temp files:\")\n",
    "        for file in self.temp_file_paths:\n",
    "            rmtree(path=file, ignore_errors=True)\n",
    "\n",
    "    # HDFS Client\n",
    "    def get_client(self, block_params=None, connection_params=None):\n",
    "        try:\n",
    "            kerb_auth = False\n",
    "            method = \"https\"\n",
    "\n",
    "            if \"https\" in connection_params:\n",
    "                if connection_params[\"https\"]:\n",
    "                    method = \"https\"\n",
    "                else:\n",
    "                    method = \"http\"\n",
    "\n",
    "            host_name = connection_params[\"hostName\"]\n",
    "            port = connection_params[\"port\"]\n",
    "\n",
    "            if 'kerberos' in connection_params:\n",
    "                kerb_auth = bool(connection_params['kerberos'])\n",
    "\n",
    "            if kerb_auth:\n",
    "                principal = generate_ticket_granting_ticket(block_params, connection_params[\"authName\"])\n",
    "                session = requests.Session()\n",
    "                session.verify = False\n",
    "                full_host = \"%s://%s:%s\" % (method, host_name, port)\n",
    "                client = KerberosClient(url=full_host, session=session, mutual_auth='OPTIONAL', principal=principal)\n",
    "                client.list('/')\n",
    "                return client\n",
    "            else:\n",
    "                hadoop_host = host_name + \":\" + port\n",
    "                client = InsecureClient(\"http://\" + hadoop_host)\n",
    "                client.list('/')\n",
    "                return client\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Error Occurred While Connecting to HDFS With Given Connection Details\")\n",
    "            raise e\n",
    "\n",
    "    def get_df(self, spark_session, data_source, block_params, path):\n",
    "        if data_source['queueTopicName'] and len(data_source['queueTopicName']) > 3:\n",
    "            return self.get_streaming_df(spark_session, path)\n",
    "        elif data_source['type'] == 'hdfs':\n",
    "            return self.get_hdfs_df(spark_session, data_source, self.hdfs_client)\n",
    "        elif data_source['type'] == 'oracle':\n",
    "            return self.get_oracle_df(spark_session, data_source, block_params)\n",
    "        else:\n",
    "            raise Exception('please provide a valid type')\n",
    "\n",
    "    def get_hdfs_df(self, spark_session, data_source, hdfs_connection):\n",
    "        self.logger.info(\"Reading HDFS file to local\")\n",
    "        local_file_path = self.base_temp_path + str(time.time()) + '.csv'\n",
    "        hdfs_connection.download(data_source['fileWithFullPath'], local_file_path, n_threads=-1,\n",
    "                                 chunk_size=5000000, overwrite=True)\n",
    "        self.logger.info(\"Creating dataframe from HDFS\")\n",
    "        self.temp_file_paths.append(local_file_path)\n",
    "        return spark_session.read.format(\"csv\").option(\"header\", data_source['header']) \\\n",
    "            .option(\"inferSchema\", \"true\").option(\"delimiter\", data_source['delimiter']) \\\n",
    "            .load(local_file_path)\n",
    "\n",
    "    def get_oracle_df(self, spark_session, data_source, block_params):\n",
    "        self.logger.info(\"Creating Oracle dataframe\")\n",
    "        credentials = get_oracle_creds(user_id=block_params[\"userAuthToken\"],\n",
    "                                       authentication_name=data_source['connection_name'])\n",
    "        url = \"jdbc:oracle:thin:@%s:%s:%s\" % (credentials[\"host\"], credentials[\"port\"], credentials[\"sid\"])\n",
    "        return spark_session.read.format(\"jdbc\") \\\n",
    "            .options(url=url\n",
    "                     , driver=\"oracle.jdbc.driver.OracleDriver\"\n",
    "                     , dbtable=data_source['query']\n",
    "                     , fetchSize=1000000\n",
    "                     , user=credentials[\"username\"]\n",
    "                     , password=credentials[\"password\"]).load()\n",
    "\n",
    "    def get_streaming_path(self, data_source):\n",
    "        topic_name = data_source['queueTopicName']\n",
    "        consumer_pool = {\n",
    "            \"count\": 1,\n",
    "            \"groupId\": str(uuid.uuid4()),\n",
    "            \"registerId\": \"\",\n",
    "            \"topicsListToSubscribe\": [\n",
    "                topic_name\n",
    "            ]\n",
    "        }\n",
    "        try:\n",
    "            consumer_pool_res = self.kafka_api_instance.create_consumer_list_using_post(consumer_pool)\n",
    "            channel = consumer_pool_res.result\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Error Trying To Create a Consumer Of Topic:\" + str(topic_name))\n",
    "            self.block_status = \"FAILED\"\n",
    "            raise e\n",
    "        req = {\"topicName\": topic_name}\n",
    "        schema = self.kafka_api_instance.get_topic_meta_using_post(req)\n",
    "        schema = json.loads(json.loads(schema.result)[\"schema\"])\n",
    "        self.logger.info(f\"Schema: {schema}\")\n",
    "\n",
    "        f_path = self.base_temp_path + str(time.time())\n",
    "        if os.path.exists(f_path):\n",
    "            rmtree(f_path)\n",
    "        read_stream_thread = Thread(target=self.read_records,\n",
    "                                    args=(topic_name, schema, self.kafka_api_instance, f_path, channel, self.logger))\n",
    "        read_stream_thread.start()\n",
    "        self.temp_file_paths.append(f_path)\n",
    "        return f_path, read_stream_thread\n",
    "\n",
    "    def get_streaming_df(self, spark, path):\n",
    "        return spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\").option(\"delimiter\", \",\") \\\n",
    "            .load(path)\n",
    "\n",
    "    def read_records(self, topic, schema, kafka_api_instance, file_path=None, channel=None, logger=None):\n",
    "        try:\n",
    "            logger.info(f'Started reading topic {topic}')\n",
    "            read_msgs_channel = {\n",
    "                \"channelId\": channel,\n",
    "                \"registerId\": \"\"\n",
    "            }\n",
    "            t_records = 0\n",
    "            with open(file_path, 'a', buffering=50 * (1024 ** 2)) as writer:\n",
    "                writer.write(\",\".join(schema.keys()))\n",
    "                while True:\n",
    "                    read_msgs_res = kafka_api_instance.read_messages_from_topic_using_post(read_msgs_channel)\n",
    "                    msgs = read_msgs_res.result\n",
    "                    logger.info(f'Read: {str(len(msgs))}, Total: {t_records}')\n",
    "\n",
    "                    if len(msgs) == 0:\n",
    "                        logger.info('Zero Messages')\n",
    "                        topic = {\"topicName\": topic}\n",
    "                        res = kafka_api_instance.get_producer_status_using_post(topic)\n",
    "                        logger.info('Zero messages: ' + json.dumps(res.result))\n",
    "                        if not res.result['value']:\n",
    "                            break\n",
    "                    # logger.info(f'message(0): {msgs[0]}')\n",
    "                    for msg in msgs:\n",
    "                        t_records = t_records + 1\n",
    "                        writer.write('\\n')\n",
    "                        my_string = ','.join(map(str, literal_eval(msg)))\n",
    "                        writer.write(my_string)\n",
    "\n",
    "            logger.info(f'Done writing topic: {topic} to file: {file_path} Records: {t_records}')\n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "\n",
    "    def file_exits(self, hdfs_connection, file_path):\n",
    "        self.logger.debug(\"Inside the file exists check method\")\n",
    "        try:\n",
    "            return hdfs_connection.status(hdfs_path=file_path, strict=True)\n",
    "        except Exception as e:\n",
    "            return False\n",
    "\n",
    "    def delete_file(self, hdfs_connection, file_path):\n",
    "        self.logger.debug(\"Inside delete HDFS file method\")\n",
    "        try:\n",
    "            return hdfs_connection.delete(file_path)\n",
    "        except Exception as e:\n",
    "            return False\n",
    "\n",
    "    def block_folder_write(self, hdfs_connection, local_file_path, upload_path, overwrite: bool):\n",
    "        self.logger.info(\"Inside the write method\")\n",
    "        hdfs_connection.upload(upload_path, local_file_path, n_threads=4,\n",
    "                               chunk_size=5000000, cleanup=True, overwrite=overwrite)\n",
    "        self.logger.info(\"Done writing\")\n",
    "\n",
    "    @async\n",
    "    def stream(self):\n",
    "        print('Stream function says: Hello, world!')\n",
    "        try:\n",
    "            schema = self.get_df_schema(self.join_df)\n",
    "            print(f\"Join Schema: {schema}\")\n",
    "            self.set_schema(self.output_topic, schema)\n",
    "            acc_counter = self.spark.sparkContext.accumulator(0)\n",
    "            # kafka method\n",
    "            #\n",
    "            # def do_count(x):\n",
    "            #     acc_counter.add(1)\n",
    "            #     return x\n",
    "            # adf_count = self.join_df.rdd.map(do_count).toDF()\n",
    "            # adf_count.selectExpr(\"to_json(struct(*)) AS value\") \\\n",
    "            #     .write \\\n",
    "            #     .format(\"kafka\") \\\n",
    "            #     .option(\"kafka.bootstrap.servers\", os.environ['KAFKA_LIST']) \\\n",
    "            #     .option(\"topic\", self.output_topic) \\\n",
    "            #     .save()\n",
    "\n",
    "            ###  Normal forEach part\n",
    "\n",
    "            # def customFunction(rows):\n",
    "            #         for row in rows:\n",
    "            #             acc_counter.add(1)\n",
    "            #             my_list = list(row.asDict().values())\n",
    "            #             # csv_str = \",\".join(str(item) for item in my_list)\n",
    "            #             self.producer.send(my_list)\n",
    "            #\n",
    "            # self.join_df.rdd.foreachPartition(customFunction)\n",
    "\n",
    "            ### using files\n",
    "            print(f\"Partfiles = {self.part_files}\")\n",
    "            for f in self.part_files:\n",
    "                self.logger.info(f'{f} appending to stream\\n')\n",
    "                with open(str(f), 'r') as in_file:\n",
    "                    for line in in_file:\n",
    "                        acc_counter.add(1)\n",
    "                        self.producer.send(line.split(\",\"))\n",
    "\n",
    "            total_records = acc_counter.value\n",
    "            self.logger.info(f\"Total records: {total_records}\")\n",
    "\n",
    "            # Update meta to KML\n",
    "            temp_error = {\n",
    "                \"noOfIgnoredRecords\": 0,\n",
    "                \"errorRecords\": 0\n",
    "            }\n",
    "            ui_info = {\"info\": \"\", \"error\": \"\"}\n",
    "            meta_data = {\n",
    "                \"schema\": json.dumps(schema),\n",
    "                \"readerInfo\": json.dumps({\"noOfRecordsRead\": total_records}),\n",
    "                \"readerInfoError\": json.dumps(temp_error),\n",
    "                \"ui_info\": json.dumps(ui_info)\n",
    "            }\n",
    "\n",
    "            self.set_meta_data(self.output_topic, meta_data=meta_data)\n",
    "            self.logger.info(f\"Total records read: {str(total_records)}\")\n",
    "        except Exception as e:\n",
    "            self.block_status = \"FAILED\"\n",
    "            some_track = traceback.format_exc()\n",
    "            self.logger.error(f\"Error: {str(e)} \\n {str(some_track)}\")\n",
    "            print(f\"Error: {str(e)} \\n {str(some_track)}\")\n",
    "            raise e\n",
    "        finally:\n",
    "            self.producer.close()\n",
    "\n",
    "        self.clean_temp_files()\n",
    "\n",
    "    def get_df_schema(self, df):\n",
    "        try:\n",
    "            json_schema = df.schema.json()\n",
    "            json_schema = json.loads(json_schema)\n",
    "\n",
    "            b2s_dict = {}\n",
    "\n",
    "            for i, val in enumerate(json_schema['fields']):\n",
    "                if val['type'] == 'integer':\n",
    "                    b2s_dict[val['name'].upper()] = {'order': i + 1, 'active': True, 'type': 'IntegerType()'}\n",
    "                if val['type'] == 'string':\n",
    "                    b2s_dict[val['name'].upper()] = {'order': i + 1, 'active': True, 'type': 'StringType()'}\n",
    "                if val['type'] == 'float' or val['type'] == 'double':\n",
    "                    b2s_dict[val['name'].upper()] = {'order': i + 1, 'active': True, 'type': 'FloatType()'}\n",
    "            return b2s_dict\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG]:  Message Handler Created\n",
      "[INFO]:  Set Params Complete\n"
     ]
    }
   ],
   "source": [
    "join_blk = MyBlock()\n",
    "\n",
    "bp = {\n",
    "    'userAuthToken': '6d72d2cf-45b6-464c-9811-d6cf21574ece'\n",
    "}\n",
    "\n",
    "ip = {\n",
    "    \"LeftDataSource\": {\n",
    "      \"connection_name\": \"ora_con\",\n",
    "      \"query\": \"(SELECT * from tables)\",\n",
    "      \"type\": \"oracle\",\n",
    "      \"fileWithFullPath\": \"/data/hdfs-nfs/Data/6d72d2cf-45b6-464c-9811-d6cf21574ece/100000SalesRecords1.csv\",\n",
    "      \"header\": True,\n",
    "      \"delimiter\": \",\",\n",
    "      \"queueTopicName\": \"\"\n",
    "    },\n",
    "    \"RightDataSource\": {\n",
    "      \"connection_name\": \"\",\n",
    "      \"query\": \"\",\n",
    "      \"type\": \"hdfs\",\n",
    "      \"fileWithFullPath\": \"/data/hdfs-nfs/Data/6d72d2cf-45b6-464c-9811-d6cf21574ece/100000SalesRecords1.csv\",\n",
    "      \"header\": True,\n",
    "      \"delimiter\": \",\",\n",
    "      \"queueTopicName\": \"\"\n",
    "    },\n",
    "    \"DataTarget\":{\n",
    "        \"join_query\": \"SELECT * from left l JOIN right r ON l.`Order ID`=r.`Order ID`\",\n",
    "        \"stream\": False,\n",
    "        \"filePath\": \"/data/test_phani/join_out_note.csv\",\n",
    "        \"delimiter\": \",\",\n",
    "        \"overwrite\": True\n",
    "    },\n",
    "    \"ConnectionParams\":{\n",
    "        \n",
    "        \"hostName\": \"172.16.109.117\",\n",
    "        \"port\": \"9870\",\n",
    "        \"kerberos\": False,\n",
    "        \"authName\": \"\",\n",
    "        \"https\": False\n",
    "\n",
    "    },\n",
    "    \"SparkConf\":{\n",
    "      \"spark.memory.fraction\": 0.9,\n",
    "      \"spark.storage.memoryFraction\": 0.3,\n",
    "      \"spark.jars\":\"/home/jovyan/work/data/spark-libs/ojdbc8-12.2.0.1.jar\"\n",
    "    }\n",
    "}\n",
    "\n",
    "join_blk.set_params(ip,bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]:  Run function says: SparkJoin\n",
      "[INFO]:  Inputs available: {'LeftDataSource': {'connection_name': 'ora_con', 'query': '(SELECT * from tables)', 'type': 'oracle', 'fileWithFullPath': '/data/hdfs-nfs/Data/6d72d2cf-45b6-464c-9811-d6cf21574ece/100000SalesRecords1.csv', 'header': True, 'delimiter': ',', 'queueTopicName': ''}, 'RightDataSource': {'connection_name': '', 'query': '', 'type': 'hdfs', 'fileWithFullPath': '/data/hdfs-nfs/Data/6d72d2cf-45b6-464c-9811-d6cf21574ece/100000SalesRecords1.csv', 'header': True, 'delimiter': ',', 'queueTopicName': ''}, 'DataTarget': {'join_query': 'SELECT * from left l JOIN right r ON l.`Order ID`=r.`Order ID`', 'stream': False, 'filePath': '/data/test_phani/join_out_note.csv', 'delimiter': ',', 'overwrite': True}, 'ConnectionParams': {'hostName': '172.16.109.117', 'port': '9870', 'kerberos': False, 'authName': '', 'https': False}, 'SparkConf': {'spark.memory.fraction': 0.9, 'spark.storage.memoryFraction': 0.3, 'spark.jars': '/home/jovyan/work/data/spark-libs/ojdbc8-12.2.0.1.jar'}}\n",
      "[INFO]:  False\n",
      "[INFO]:  Creating Oracle dataframe\n",
      "[ERROR]:  Error: An error occurred while calling o73.load.\n",
      ": java.sql.SQLSyntaxErrorException: ORA-00942: table or view does not exist\n",
      "\n",
      "\tat oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:450)\n",
      "\tat oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:399)\n",
      "\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1059)\n",
      "\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:522)\n",
      "\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:257)\n",
      "\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:587)\n",
      "\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:225)\n",
      "\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:53)\n",
      "\tat oracle.jdbc.driver.T4CPreparedStatement.executeForDescribe(T4CPreparedStatement.java:774)\n",
      "\tat oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:925)\n",
      "\tat oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1111)\n",
      "\tat oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:4798)\n",
      "\tat oracle.jdbc.driver.OraclePreparedStatement.executeQuery(OraclePreparedStatement.java:4845)\n",
      "\tat oracle.jdbc.driver.OraclePreparedStatementWrapper.executeQuery(OraclePreparedStatementWrapper.java:1501)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:60)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.<init>(JDBCRelation.scala:115)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:52)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:340)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:164)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      " \n",
      " trace: Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-c1c24453e997>\", line 76, in run\n",
      "    self.left_df = self.get_df(self.spark, self.input_dict['LeftDataSource'], self.block_params, left_path)\n",
      "  File \"<ipython-input-2-c1c24453e997>\", line 226, in get_df\n",
      "    return self.get_oracle_df(spark_session, data_source, block_params)\n",
      "  File \"<ipython-input-2-c1c24453e997>\", line 252, in get_oracle_df\n",
      "    , password=credentials[\"password\"]).load()\n",
      "  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 172, in load\n",
      "    return self._df(self._jreader.load())\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o73.load.\n",
      ": java.sql.SQLSyntaxErrorException: ORA-00942: table or view does not exist\n",
      "\n",
      "\tat oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:450)\n",
      "\tat oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:399)\n",
      "\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1059)\n",
      "\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:522)\n",
      "\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:257)\n",
      "\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:587)\n",
      "\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:225)\n",
      "\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:53)\n",
      "\tat oracle.jdbc.driver.T4CPreparedStatement.executeForDescribe(T4CPreparedStatement.java:774)\n",
      "\tat oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:925)\n",
      "\tat oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1111)\n",
      "\tat oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:4798)\n",
      "\tat oracle.jdbc.driver.OraclePreparedStatement.executeQuery(OraclePreparedStatement.java:4845)\n",
      "\tat oracle.jdbc.driver.OraclePreparedStatementWrapper.executeQuery(OraclePreparedStatementWrapper.java:1501)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:60)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.<init>(JDBCRelation.scala:115)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:52)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:340)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:164)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n",
      "[ERROR]:  An error occurred while calling o73.load.\n",
      ": java.sql.SQLSyntaxErrorException: ORA-00942: table or view does not exist\n",
      "\n",
      "\tat oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:450)\n",
      "\tat oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:399)\n",
      "\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1059)\n",
      "\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:522)\n",
      "\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:257)\n",
      "\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:587)\n",
      "\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:225)\n",
      "\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:53)\n",
      "\tat oracle.jdbc.driver.T4CPreparedStatement.executeForDescribe(T4CPreparedStatement.java:774)\n",
      "\tat oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:925)\n",
      "\tat oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1111)\n",
      "\tat oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:4798)\n",
      "\tat oracle.jdbc.driver.OraclePreparedStatement.executeQuery(OraclePreparedStatement.java:4845)\n",
      "\tat oracle.jdbc.driver.OraclePreparedStatementWrapper.executeQuery(OraclePreparedStatementWrapper.java:1501)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:60)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.<init>(JDBCRelation.scala:115)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:52)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:340)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:164)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o73.load.\n: java.sql.SQLSyntaxErrorException: ORA-00942: table or view does not exist\n\n\tat oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:450)\n\tat oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:399)\n\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1059)\n\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:522)\n\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:257)\n\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:587)\n\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:225)\n\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:53)\n\tat oracle.jdbc.driver.T4CPreparedStatement.executeForDescribe(T4CPreparedStatement.java:774)\n\tat oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:925)\n\tat oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1111)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:4798)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeQuery(OraclePreparedStatement.java:4845)\n\tat oracle.jdbc.driver.OraclePreparedStatementWrapper.executeQuery(OraclePreparedStatementWrapper.java:1501)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:60)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.<init>(JDBCRelation.scala:115)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:52)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:340)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:164)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/work/platform-libs/initialize.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjoin_blk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/platform-libs/initialize.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error: {str(e)} \\n trace: {some_track}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_temp_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/platform-libs/initialize.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhdfs_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ConnectionParams'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LeftDataSource'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RightDataSource'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/platform-libs/initialize.py\u001b[0m in \u001b[0;36mget_df\u001b[0;34m(self, spark_session, data_source, block_params, path)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_hdfs_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhdfs_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdata_source\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'oracle'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_oracle_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'please provide a valid type'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/platform-libs/initialize.py\u001b[0m in \u001b[0;36mget_oracle_df\u001b[0;34m(self, spark_session, data_source, block_params)\u001b[0m\n\u001b[1;32m    250\u001b[0m                      \u001b[0;34m,\u001b[0m \u001b[0mfetchSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                      \u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"username\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                      , password=credentials[\"password\"]).load()\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_streaming_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o73.load.\n: java.sql.SQLSyntaxErrorException: ORA-00942: table or view does not exist\n\n\tat oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:450)\n\tat oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:399)\n\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1059)\n\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:522)\n\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:257)\n\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:587)\n\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:225)\n\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:53)\n\tat oracle.jdbc.driver.T4CPreparedStatement.executeForDescribe(T4CPreparedStatement.java:774)\n\tat oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:925)\n\tat oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1111)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:4798)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeQuery(OraclePreparedStatement.java:4845)\n\tat oracle.jdbc.driver.OraclePreparedStatementWrapper.executeQuery(OraclePreparedStatementWrapper.java:1501)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:60)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.<init>(JDBCRelation.scala:115)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:52)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:340)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:164)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "join_blk.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Region_1',\n",
       " 'Country_1',\n",
       " 'Item Type_1',\n",
       " 'Sales Channel_1',\n",
       " 'Order Priority_1',\n",
       " 'Order Date_1',\n",
       " 'Order ID_1',\n",
       " 'Ship Date_1',\n",
       " 'Units Sold_1',\n",
       " 'Unit Price_1',\n",
       " 'Unit Cost_1',\n",
       " 'Total Revenue_1',\n",
       " 'Total Cost_1',\n",
       " 'Total Profit_1',\n",
       " 'Region',\n",
       " 'Country',\n",
       " 'Item Type',\n",
       " 'Sales Channel',\n",
       " 'Order Priority',\n",
       " 'Order Date',\n",
       " 'Order ID',\n",
       " 'Ship Date',\n",
       " 'Units Sold',\n",
       " 'Unit Price',\n",
       " 'Unit Cost',\n",
       " 'Total Revenue',\n",
       " 'Total Cost',\n",
       " 'Total Profit']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_blk.join_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " kafka-clients-2.4.0.jar  'ojdbc8-12.2.0.1 (1).jar'\n",
      " ojdbc6-11.2.0.3.jar       ojdbc8-12.2.0.1.jar\n",
      " ojdbc6.jar                spark-sql-kafka-0-10_2.11-2.4.5.jar\n",
      " ojdbc7.jar                spark-sql-kafka-0-10_2.12-2.4.5.jar\n"
     ]
    }
   ],
   "source": [
    "ls /home/jovyan/work/data/spark-libs/ojdbc8-12.2.0.1.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'LC_ALL': 'en_US.UTF-8',\n",
       "        'JUPYTERHUB_CLIENT_ID': 'jupyterhub-user-6d72d2cf-45b6-464c-9811-d6cf21574ece',\n",
       "        'LANG': 'en_US.UTF-8',\n",
       "        'HOSTNAME': 'bb2f5fb24eb0',\n",
       "        'NB_UID': '1000',\n",
       "        'CONDA_DIR': '/opt/conda',\n",
       "        'CPU_LIMIT': '2.0',\n",
       "        'JUPYTERHUB_BASE_URL': '/',\n",
       "        'MESOS_NATIVE_LIBRARY': '/usr/local/lib/libmesos.so',\n",
       "        'PWD': '/home/jovyan',\n",
       "        'HOME': '/home/jovyan',\n",
       "        'BB_EXECUTION': 'JUPYTER',\n",
       "        'JUPYTERHUB_USER': '6d72d2cf-45b6-464c-9811-d6cf21574ece',\n",
       "        'DEBIAN_FRONTEND': 'noninteractive',\n",
       "        'SPARK_HOME': '/usr/local/spark',\n",
       "        'NB_USER': 'jovyan',\n",
       "        'HADOOP_VERSION': '2.7',\n",
       "        'JUPYTERHUB_SERVICE_PREFIX': '/user/6d72d2cf-45b6-464c-9811-d6cf21574ece/',\n",
       "        'SHELL': '/bin/bash',\n",
       "        'SPARK_OPTS': '--driver-java-options=-Xms1024M --driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info',\n",
       "        'MEM_LIMIT': '2147483648',\n",
       "        'APACHE_SPARK_VERSION': '2.3.1',\n",
       "        'JUPYTERHUB_API_URL': 'http://47a0c8bd0b6c:8081/hub/api',\n",
       "        'SHLVL': '0',\n",
       "        'LANGUAGE': 'en_US.UTF-8',\n",
       "        'PYTHONPATH': '/usr/local/spark/python:/usr/local/spark/python/lib/py4j-0.10.7-src.zip',\n",
       "        'JUPYTERHUB_HOST': '',\n",
       "        'JPY_API_TOKEN': '44287e98095343b689bd4259ded2ee24',\n",
       "        'XDG_CACHE_HOME': '/home/jovyan/.cache/',\n",
       "        'JUPYTERHUB_OAUTH_CALLBACK_URL': '/user/6d72d2cf-45b6-464c-9811-d6cf21574ece/oauth_callback',\n",
       "        'NB_GID': '100',\n",
       "        'JUPYTERHUB_API_TOKEN': '44287e98095343b689bd4259ded2ee24',\n",
       "        'PATH': '/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\n",
       "        'BBSDK_CONFIG': '/home/jovyan/work/data/config_notebook.json',\n",
       "        'EXECUTION_ENV': 'NOTEBOOK',\n",
       "        'MINICONDA_VERSION': '4.5.4',\n",
       "        'JPY_PARENT_PID': '6',\n",
       "        'TERM': 'xterm-color',\n",
       "        'CLICOLOR': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://ipykernel.pylab.backend_inline'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream(self):\n",
    "    print('Stream function says: Hello, world!')\n",
    "    try:\n",
    "        schema = self.get_df_schema(self.df)\n",
    "        print(f\"Schema: {schema}\")\n",
    "        self.set_schema(self.output_topic, schema)\n",
    "        acc_counter = self.spark.sparkContext.accumulator(0)\n",
    "        # Write dataframe to files\n",
    "        self.join_df.write.mode(\"overwrite\").option(\"header\", \"false\").csv(temp_fp)\n",
    "        part_files = list(pathlib.Path(temp_fp).glob('*.csv'))\n",
    "        part_files.sort()\n",
    "\n",
    "        # if possible do above steps in run method\n",
    "        # Using files\n",
    "        print(f\"Partfiles = {self.part_files}\")\n",
    "        for f in self.part_files:\n",
    "            self.logger.info(f'{f} appending to stream\\n')\n",
    "            with open(str(f), 'r') as in_file:\n",
    "                for line in in_file:\n",
    "                    acc_counter.add(1)\n",
    "                    self.producer.send(line.split(\",\"))\n",
    "\n",
    "        total_records = acc_counter.value\n",
    "        self.logger.info(f\"Total records: {total_records}\")\n",
    "\n",
    "        # Update meta to KML\n",
    "        temp_error = {\n",
    "            \"noOfIgnoredRecords\": 0,\n",
    "            \"errorRecords\": 0\n",
    "        }\n",
    "        ui_info = {\"info\": \"\", \"error\": \"\"}\n",
    "        meta_data = {\n",
    "            \"schema\": json.dumps(schema),\n",
    "            \"readerInfo\": json.dumps({\"noOfRecordsRead\": total_records}),\n",
    "            \"readerInfoError\": json.dumps(temp_error),\n",
    "            \"ui_info\": json.dumps(ui_info)\n",
    "        }\n",
    "\n",
    "        self.set_meta_data(self.output_topic, meta_data=meta_data)\n",
    "        self.logger.info(f\"Total records read: {str(total_records)}\")\n",
    "    except Exception as e:\n",
    "        self.block_status = \"FAILED\"\n",
    "        some_track = traceback.format_exc()\n",
    "        self.logger.error(f\"Error: {str(e)} \\n {str(some_track)}\")\n",
    "        print(f\"Error: {str(e)} \\n {str(some_track)}\")\n",
    "        raise e\n",
    "    finally:\n",
    "        self.producer.close()\n",
    "\n",
    "    self.clean_temp_files()\n",
    "\n",
    "def get_df_schema(self, df):\n",
    "    try:\n",
    "        json_schema = df.schema.json()\n",
    "        json_schema = json.loads(json_schema)\n",
    "\n",
    "        b2s_dict = {}\n",
    "\n",
    "        for i, val in enumerate(json_schema['fields']):\n",
    "            if val['type'] == 'integer':\n",
    "                b2s_dict[val['name'].upper()] = {'order': i + 1, 'active': True, 'type': 'IntegerType()'}\n",
    "            if val['type'] == 'string':\n",
    "                b2s_dict[val['name'].upper()] = {'order': i + 1, 'active': True, 'type': 'StringType()'}\n",
    "            if val['type'] == 'float' or val['type'] == 'double':\n",
    "                b2s_dict[val['name'].upper()] = {'order': i + 1, 'active': True, 'type': 'FloatType()'}\n",
    "        return b2s_dict\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
